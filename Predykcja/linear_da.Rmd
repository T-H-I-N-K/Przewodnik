# Liniowa i kwadratowa klasyfikacja

## Jak to działa?

Podobnie jak w przypadku klasyfikatora Naiwny Bayes, interesuje nas modelowanie rozkładu $y|X$. Z twierdzenia Bayesa mamy
$$
P(y_i = 1| X_i = x_i) = \frac{P(X_i = x_i | y_i = 1)P(y_i = 1)}{\sum_k P(X_i = x_i | y_i = k)P(y_i = k)}
$$

Log-szansa ma postać
$$
\log (odds_i) = \log \frac{P(X_i = x_i | y_i = 1)P(y_i = 1)}{P(X_i = x_i | y_i = 0)P(y_i = 0)}  
$$
W przypadku Liniowej Analizy Dyskryminacyjnej (LDA, ang. Linear Discriminant Analysis), zakłada się, że
$$
X_i | (y_i = k) \sim \mathcal N (\mu_k, \Sigma)
$$
A więc log szansę można wyrazić jako 

$$
\log (odds_i) = \log \frac{(2\pi)^{-1/2} |\Sigma|^{-1/2} \exp\left(-\frac 12 (x_i - \mu_1)^T\Sigma^{-1}(x_i - \mu_1)\right) P(y_i = 1)}{(2\pi)^{-1/2} |\Sigma|^{-1/2} \exp\left(-\frac 12 (x_i - \mu_0)^T\Sigma^{-1}(x_i - \mu_0)\right) P(y_i = 0)}  
$$
Kilka prostch przekształceń i mamy
$$
\log (odds_i) =  
 x_i^T\Sigma^{-1} (\mu_1 - \mu_0)
-\frac 12 \mu_1^T\Sigma^{-1}\mu_1 + \frac 12 \mu_0^T\Sigma^{-1}\mu_0+
\log \frac{ P(y_i = 1)}{ P(y_i = 0)} =
x_i^T A + c
$$
Jak widzimy, szansa w przypadku metody LDA wyraża się liniową funkcją zmiennych $x_i$. Macierz współczynników $A$ i $c$ wyznacza się na bazie ocen $\hat\mu_k$ i $\hat\Sigma$.

Metoda kwadratowa rózni się od liniowej tylko tym, że dla każdej klasy estymuje się osobną macierz kowariancji.

$$
X_i | (y_i = k) \sim \mathcal N (\mu_k, \Sigma_k)
$$

Gdy dla takiej gęstości policzymy log szansę to okaże się, że jest ona kwadratową funkcją $x_i$.

Gdy klas jest dużo, estymacja macierzy kowariancji o rozmiarach $pxp$ jest obarczona dużym błędem. Dlatego rozważa się też regularyzowaną estymacie macierzy kowariancji w grupach, tak że
$$
\Sigma_k^r = \alpha \Sigma + (1-\alpha) \Sigma_k.
$$



## Jak to zrobić w R?



